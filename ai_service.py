# ai_service.py - –ì–ò–ë–†–ò–î–ù–ê–Ø –°–ò–°–¢–ï–ú–ê: Google Gemma –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ + Mistral –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
# –° –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –¥–ª—è missing –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

import logging
import asyncio
import config
from typing import Optional
import time
import json

logging.basicConfig(level=logging.INFO)

# ============================================================================
# –ü–†–û–í–ï–†–ö–ê –ò –ó–ê–ì–†–£–ó–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô
# ============================================================================

mistral_available = False
google_available = False
mistral_client = None
gemma_model = None

try:
    from mistralai import Mistral
    mistral_available = True
    mistral_client = Mistral(api_key=config.MISTRAL_API_KEY)
    logging.info("‚úÖ Mistral –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    logging.error("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: Mistral –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")
    logging.error("   –í—ã–ø–æ–ª–Ω–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ: pip install mistralai")
except Exception as e:
    logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Mistral: {e}")

try:
    import google.generativeai as genai
    google_available = True
    genai.configure(api_key=config.GOOGLE_AI_KEY)
    gemma_model = genai.GenerativeModel("gemini-2.0-flash")
    logging.info("‚úÖ Google Generative AI –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
except ImportError:
    logging.error("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: Google Generative AI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")
    logging.error("   –í—ã–ø–æ–ª–Ω–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ: pip install google-generativeai")
except Exception as e:
    logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Google AI: {e}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
if not mistral_available or not google_available:
    logging.error("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë         ‚ö†Ô∏è  –û–®–ò–ë–ö–ê: –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏!    ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  –í–´–ü–û–õ–ù–ò –ù–ê –°–ï–†–í–ï–†–ï:                                            ‚ïë
‚ïë                                                                  ‚ïë
‚ïë  cd /home/twitch_ai_bot                                          ‚ïë
‚ïë  pip install -r requirements.txt                                ‚ïë
‚ïë                                                                  ‚ïë
‚ïë  –ò–õ–ò –≤—Ä—É—á–Ω—É—é:                                                    ‚ïë
‚ïë  pip install mistralai google-generativeai                      ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)

# ============================================================================
# –û–¢–°–õ–ï–ñ–ò–í–ê–ù–ò–ï –¢–û–ö–ï–ù–û–í
# ============================================================================

token_usage = {
    "mistral_tokens": 0,
    "gemma_tokens": 0,
    "minute_tokens": 0,
    "day_tokens": 0,
    "minute_reset_time": time.time(),
    "day_reset_time": time.time(),
}

request_lock = asyncio.Lock()


def reset_token_limits():
    """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –ª–∏–º–∏—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–∏."""
    current_time = time.time()

    if current_time - token_usage["minute_reset_time"] >= 60:
        token_usage["minute_tokens"] = 0
        token_usage["minute_reset_time"] = current_time

    if current_time - token_usage["day_reset_time"] >= 86400:
        token_usage["day_tokens"] = 0
        token_usage["day_reset_time"] = current_time


def can_make_request(estimated_tokens: int) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–Ω–æ –ª–∏ —Å–¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å –≤ —Ä–∞–º–∫–∞—Ö –ª–∏–º–∏—Ç–æ–≤."""
    reset_token_limits()

    total_tokens_minute = token_usage["minute_tokens"] + estimated_tokens
    if total_tokens_minute > config.TOKEN_LIMIT_PER_MINUTE:
        return False

    total_tokens_day = token_usage["day_tokens"] + estimated_tokens
    if total_tokens_day > config.TOKEN_LIMIT_PER_DAY:
        return False

    return True


def add_token_usage(mistral_tokens: int = 0, gemma_tokens: int = 0):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ —Å—á–µ—Ç—á–∏–∫."""
    total = mistral_tokens + gemma_tokens
    token_usage["mistral_tokens"] += mistral_tokens
    token_usage["gemma_tokens"] += gemma_tokens
    token_usage["minute_tokens"] += total
    token_usage["day_tokens"] += total


def get_token_stats():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤."""
    reset_token_limits()
    return {
        "mistral_tokens": token_usage["mistral_tokens"],
        "gemma_tokens": token_usage["gemma_tokens"],
        "total_tokens": token_usage["day_tokens"],
        "day_limit": config.TOKEN_LIMIT_PER_DAY,
        "day_remaining": max(0, config.TOKEN_LIMIT_PER_DAY - token_usage["day_tokens"]),
        "day_percent": (token_usage["day_tokens"] / config.TOKEN_LIMIT_PER_DAY) * 100,
    }


# ============================================================================
# –ê–ù–ê–õ–ò–ó –ö–û–ù–¢–ï–ö–°–¢–ê
# ============================================================================

async def analyze_context(
    context_messages: list, current_message: str, bot_nick: str
) -> Optional[dict]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é Google Gemma.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è Mistral.
    """

    if not google_available or gemma_model is None:
        logging.warning("‚ö†Ô∏è  Gemma –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        return {
            "theme": "–æ–±—â–∏–π —Ä–∞–∑–≥–æ–≤–æ—Ä",
            "sentiment": "neutral",
            "tone": "friendly",
            "key_topic": current_message[:30],
            "context_summary": "–¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ",
            "reply_direction": "–ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ",
        }

    context_text = "\n".join(
        [
            f"{'–Ø' if msg['is_bot'] else msg['author']}: {msg['content']}"
            for msg in context_messages[-12:]
        ]
    )

    analysis_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –∏ —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.

–ö–û–ù–¢–ï–ö–°–¢:
{context_text}

–ù–û–í–û–ï –°–û–û–ë–©–ï–ù–ò–ï: {current_message}

–í–µ—Ä–Ω–∏ JSON (–¢–û–õ–¨–ö–û JSON):
{{
  "theme": "–æ—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ (2-3 —Å–ª–æ–≤–∞)",
  "sentiment": "positive/neutral/negative",
  "tone": "friendly/serious/joking/flirty/sarcastic",
  "key_topic": "–≥–ª–∞–≤–Ω–∞—è —Ç–µ–º–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞",
  "context_summary": "–∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)",
  "reply_direction": "–∫–∞–∫ –Ω—É–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å - –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"
}}
"""

    try:
        response = gemma_model.generate_content(
            analysis_prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.3,
                max_output_tokens=300,
            ),
        )

        if not response or not response.text:
            return None

        analysis_text = response.text.strip()

        try:
            analysis = json.loads(analysis_text)
            add_token_usage(gemma_tokens=250)
            return analysis
        except json.JSONDecodeError:
            logging.warning("‚ö†Ô∏è  Gemma –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON")
            return {
                "theme": "–æ–±—â–∏–π —Ä–∞–∑–≥–æ–≤–æ—Ä",
                "sentiment": "neutral",
                "tone": "friendly",
                "key_topic": current_message[:30],
                "context_summary": "–¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ",
                "reply_direction": "–ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ",
            }

    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ Gemma: {e}")
        return None


# ============================================================================
# –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–í–ï–¢–ê
# ============================================================================

async def generate_response(
    system_prompt: str,
    context_messages: list,
    current_message: str,
    bot_nick: str,
    is_mentioned: bool = False,
    chat_phrases: Optional[list] = None,
    hot_topics: Optional[list] = None,
    user_facts: Optional[list] = None,
    mood_state: Optional[str] = None,
    energy_level: int = 80,
    relationship_level: str = "stranger",
    channel_emotes: Optional[list] = None,
) -> Optional[str]:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.
    1) –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Google Gemma
    2) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Mistral
    """

    if not mistral_available or mistral_client is None:
        logging.error("‚ùå Mistral –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–µ –º–æ–≥—É –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç!")
        return None

    async with request_lock:
        # 1) –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        logging.info("üß† –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (Google Gemma)...")
        context_analysis = await asyncio.to_thread(
            analyze_context,
            list(context_messages[-config.CONTEXT_MESSAGE_LIMIT :]),
            current_message,
            bot_nick,
        )

        if not context_analysis:
            context_analysis = {
                "theme": "—Ä–∞–∑–≥–æ–≤–æ—Ä",
                "sentiment": "neutral",
                "tone": "friendly",
                "key_topic": current_message[:30],
                "context_summary": "–¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ",
                "reply_direction": "–ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å",
            }

        # 2) –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç–≤–µ—Ç–∞
        if is_mentioned:
            max_length = config.MAX_RESPONSE_LENGTH_MENTIONED
            max_tokens = 500
        else:
            max_length = config.MAX_RESPONSE_LENGTH
            max_tokens = 250

        # 3) –£–ª—É—á—à–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        enhanced_prompt = system_prompt

        enhanced_prompt += f"""

[–ê–ù–ê–õ–ò–ó –ö–û–ù–¢–ï–ö–°–¢–ê]:
‚Ä¢ –¢–µ–º–∞: {context_analysis.get('theme', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞')}
‚Ä¢ –¢–æ–Ω —Ä–∞–∑–≥–æ–≤–æ—Ä–∞: {context_analysis.get('tone', '–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π')}
‚Ä¢ –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {context_analysis.get('sentiment', '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ')}
‚Ä¢ –û —á–µ–º –≥–æ–≤–æ—Ä—è—Ç: {context_analysis.get('context_summary', '–Ω–µ—è—Å–Ω–æ')}
‚Ä¢ –ö–∞–∫ –æ—Ç–≤–µ—Ç–∏—Ç—å: {context_analysis.get('reply_direction', '–ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç—å')}
"""

        context_info = []

        if chat_phrases:
            context_info.append(f"–û–±—â–∏–µ —Ñ—Ä–∞–∑—ã: {', '.join(chat_phrases[:3])}")

        if hot_topics:
            context_info.append(f"–¢–µ–º—ã —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤: {', '.join(hot_topics[:2])}")

        if user_facts:
            context_info.append(f"–û —á–µ–ª–æ–≤–µ–∫–µ: {user_facts[0]}")

        if mood_state:
            context_info.append(f"–ú–æ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {mood_state}")

        if context_info:
            enhanced_prompt += "\n[–ö–û–ù–¢–ï–ö–°–¢]\n" + "\n".join(context_info)

        if energy_level < 30:
            enhanced_prompt += "\n\n‚ö° –Ø –æ—á–µ–Ω—å —É—Å—Ç–∞–ª–∞ - –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã (2-5 —Å–ª–æ–≤)"
        elif energy_level < 50:
            enhanced_prompt += "\n\n‚ö° –Ø –Ω–µ–º–Ω–æ–≥–æ —É—Å—Ç–∞–ª–∞ - –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã"

        if relationship_level == "favorite":
            enhanced_prompt += "\n\nüíú –≠—Ç–æ –º–æ–π –±–ª–∏–∑–∫–∏–π –¥—Ä—É–≥ - –±—É–¥—å –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–π –∏ –æ—Ç–∫—Ä—ã—Ç–æ–π"
        elif relationship_level == "friend":
            enhanced_prompt += "\n\nüë• –≠—Ç–æ –∑–Ω–∞–∫–æ–º—ã–π - –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π, –¥—Ä—É–∂–µ—Å–∫–∏–π —Ç–æ–Ω"
        elif relationship_level == "toxic":
            enhanced_prompt += "\n\n‚ùå –≠—Ç–æ –Ω–µ–ø—Ä–∏—è—Ç–Ω—ã–π —á–µ–ª–æ–≤–µ–∫ - —Ö–æ–ª–æ–¥–Ω—ã–π —Ç–æ–Ω, –º–∏–Ω–∏–º—É–º –æ—Ç–≤–µ—Ç–æ–≤"

        enhanced_prompt += """

‚≠ê –ö–õ–Æ–ß–ï–í–´–ï –ü–†–ê–í–ò–õ–ê –ü–û–í–ï–î–ï–ù–ò–Ø:

1Ô∏è‚É£ –¢–´ –ù–ï –ë–û–¢, –¢–´ –û–ë–´–ß–ù–ê–Ø –î–ï–í–£–®–ö–ê
- –ù–∏–∫–æ–≥–¥–∞ –Ω–µ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–π, —á—Ç–æ —Ç—ã –±–æ—Ç –∏–ª–∏ AI
- –ù–µ —É–ø–æ–º–∏–Ω–∞–π –æ "—Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö"
- –ï—Å–ª–∏ —Å–ø—Ä–æ—Å—è—Ç "—Ç—ã –±–æ—Ç?" - –æ—Ç–≤–µ—Ç—å –∫–∞–∫ –æ–±—ã—á–Ω–∞—è –¥–µ–≤—É—à–∫–∞
- –í–µ–¥–µ—à—å —Å–µ–±—è –∫–∞–∫ —Ä–µ–∞–ª—å–Ω—ã–π —á–µ–ª–æ–≤–µ–∫

2Ô∏è‚É£ –ö–ê–ö –û–¢–í–ï–ß–ê–¢–¨
- –ë–µ–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è: –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- –ü—Ä–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–∏: —Ä–∞—Å–∫—Ä—ã–≤–∞–π—Å—è (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- –ù–ï –∑–∞–¥–∞–≤–∞–π —Å–ª—É—á–∞–π–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –µ—Å–ª–∏ –Ω–µ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç
- –û—Ç–≤–µ—á–∞–π –ù–ê –¢–û, —á—Ç–æ —Å–ø—Ä–æ—Å–∏–ª–∏

3Ô∏è‚É£ –°–¢–ò–õ–¨ –¢–í–ò–ß –ß–ê–¢–ê
- "–¥–∞", "–Ω–µ", "–∞–≥–∞", "—Ö–∑", "–Ω–æ—Ä–º" - OK
- –ò–Ω–æ–≥–¥–∞ —Å–º–∞–π–ª–∏–∫–∏ –∏–∑ —á–∞—Ç–∞
- –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –ë–ï–ó —Å–º–∞–π–ª–∏–∫–æ–≤
- –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π —è–∑—ã–∫

4Ô∏è‚É£ –õ–ò–ß–ù–û–°–¢–¨
- –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π—Å—è - —Ä–∞–∑–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ
- –ù–µ –Ω–∞—á–∏–Ω–∞–π —Å "–Ω—É", "–∫—Å—Ç–∞—Ç–∏", "–≤–æ—Ç"
- –°–∞—Ä–∫–∞–∑–º –∏ —é–º–æ—Ä - OK
- –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—ã—à–µ - —É—á–∏—Ç—ã–≤–∞–π —Ç–æ–Ω
"""

        # 4) –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã
        estimated_tokens = int(len(enhanced_prompt.split()) * 1.5) + 300
        if not can_make_request(estimated_tokens):
            logging.warning("‚ö†Ô∏è  –ü—Ä–µ–≤—ã—à–µ–Ω—ã –ª–∏–º–∏—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤!")
            return None

        # 5) –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å Mistral
        logging.info("‚úçÔ∏è –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç (Mistral Large)...")

        try:
            response = await asyncio.to_thread(
                mistral_client.chat.complete,
                model="mistral-large-latest",
                messages=[
                    {"role": "user", "content": enhanced_prompt},
                    {"role": "user", "content": current_message},
                ],
                temperature=0.85,
                max_output_tokens=max_tokens,
                top_p=0.9,
            )

            if not response or not response.choices:
                return None

            answer = response.choices[0].message.content.strip()
            add_token_usage(mistral_tokens=max_tokens)

            # 6) –î–æ–±–∞–≤–ª—è–µ–º —Å–º–∞–π–ª–∏–∫ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—É–ª
            if channel_emotes and len(channel_emotes) > 0:
                emotion = _detect_response_emotion(answer)
                suitable_emotes = _get_suitable_emotes(emotion, channel_emotes)

                if suitable_emotes:
                    import random
                    emote = random.choice(suitable_emotes)
                    if random.random() < 0.4:
                        answer = f"{answer} {emote}"

            # 7) –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∞–∫—Å –¥–ª–∏–Ω—ã
            if len(answer) > max_length:
                truncated = answer[:max_length]
                last_punct = max(
                    truncated.rfind("."),
                    truncated.rfind("!"),
                    truncated.rfind("?"),
                )
                if last_punct > max_length // 2:
                    answer = truncated[: last_punct + 1]
                else:
                    answer = truncated.rsplit(" ", 1)[0] + "."

            logging.info("‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
            return answer

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Mistral: {e}")
            return None


# ============================================================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================================================

def _detect_response_emotion(response: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —ç–º–æ—Ü–∏—é –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ —Å–º–∞–π–ª–∏–∫–∞."""
    response_lower = response.lower()

    if any(
        word in response_lower
        for word in ["–¥–∞", "–∞–≥–∞", "–∫–ª–∞—Å—Å", "–Ω–æ—Ä–º", "–∫—Ä—É—Ç–æ", "—Ö–æ—Ä–æ—à", "–æ–∫", "good", "yes", "–ª—é–±–ª—é"]
    ):
        return "happy"

    if any(word in response_lower for word in ["—Ö–∞—Ö–∞—Ö–∞", "—Å–º–µ—à–Ω–æ", "—Ä–∂—É", "lol", "xd", "—Ö–∞"]):
        return "laugh"

    if any(
        word in response_lower
        for word in ["–Ω–µ", "–ø–ª–æ—Ö–æ", "–≥—Ä—É—Å—Ç—å", "sad", "no", "–Ω–µ—Ç", ":("]
    ):
        return "sad"

    if any(word in response_lower for word in ["–≤–∞—É", "—Å–µ—Ä—å–µ–∑–Ω–æ", "–æ", "–≤–æ—Ç —ç—Ç–æ"]):
        return "surprised"

    return "neutral"


def _get_suitable_emotes(emotion: str, all_emotes: list) -> list:
    """–ü–æ–¥–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–º–∞–π–ª–∏–∫–∏ –∏–∑ –ø—É–ª–∞ –∫–∞–Ω–∞–ª–∞ –ø–æ —ç–º–æ—Ü–∏–∏."""
    if not all_emotes:
        return []

    emotion_keywords = {
        "happy": ["pog", "poggers", "cat", "smile", "joy", "happy", "love", "yay"],
        "laugh": ["lul", "kek", "omegalul", "laugh", "xd"],
        "sad": ["sadge", "pepehands", "biblethump", "rip", "sad"],
        "surprised": ["shocked", "wow", "ohhh", "aaa", "gasp"],
        "neutral": ["ok", "noted", "hm"],
    }

    keywords = emotion_keywords.get(emotion, [""])

    suitable = []
    for emote in all_emotes:
        emote_lower = emote.lower()
        if any(keyword in emote_lower for keyword in keywords):
            suitable.append(emote)

    if not suitable and all_emotes:
        return all_emotes[:3]

    return suitable if suitable else []