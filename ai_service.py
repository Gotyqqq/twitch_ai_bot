ai_service.py - –ì–ò–ë–†–ò–î–ù–ê–Ø –°–ò–°–¢–ï–ú–ê: Google Gemma –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ + Mistral –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
import logging
import asyncio
import config
from typing import Optional
import time
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(name)

============================================================================
–ü–†–û–í–ï–†–ö–ê –ò –ó–ê–ì–†–£–ó–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô
============================================================================
mistral_available = False
google_available = False
mistral_client = None
gemma_model = None

try:
from mistralai import Mistral
mistral_available = True
mistral_client = Mistral(api_key=config.MISTRAL_API_KEY)
logger.info("‚úÖ Mistral –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
except ImportError:
logger.error("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: Mistral –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")
logger.error(" –í—ã–ø–æ–ª–Ω–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ: pip install mistralai")
except Exception as e:
logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Mistral: {e}")

try:
import google.generativeai as genai
google_available = True
genai.configure(api_key=config.GOOGLE_AI_KEY)
gemma_model = genai.GenerativeModel("gemini-2.0-flash")
logger.info("‚úÖ Google Generative AI –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
except ImportError:
logger.error("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: Google Generative AI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")
logger.error(" –í—ã–ø–æ–ª–Ω–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ: pip install google-generativeai")
except Exception as e:
logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Google AI: {e}")

============================================================================
–û–¢–°–õ–ï–ñ–ò–í–ê–ù–ò–ï –¢–û–ö–ï–ù–û–í
============================================================================
token_usage = {
"mistral_tokens": 0,
"gemma_tokens": 0,
"minute_tokens": 0,
"day_tokens": 0,
"minute_reset_time": time.time(),
"day_reset_time": time.time(),
}

request_lock = asyncio.Lock()

def add_token_usage(mistral_tokens: int = 0, gemma_tokens: int = 0):
"""–î–æ–±–∞–≤–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ —Å—á–µ—Ç—á–∏–∫."""
total = mistral_tokens + gemma_tokens
token_usage["mistral_tokens"] += mistral_tokens
token_usage["gemma_tokens"] += gemma_tokens
token_usage["minute_tokens"] += total
token_usage["day_tokens"] += total

============================================================================
–ê–ù–ê–õ–ò–ó –ö–û–ù–¢–ï–ö–°–¢–ê (Google Gemma)
============================================================================
async def analyze_context(
context_messages: list, current_message: str, bot_nick: str
) -> Optional[dict]:
"""–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é Google Gemma."""

text
if not google_available or gemma_model is None:
    logger.warning("‚ö†Ô∏è  Gemma –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    return {
        "theme": "–æ–±—â–∏–π —Ä–∞–∑–≥–æ–≤–æ—Ä",
        "sentiment": "neutral",
        "tone": "friendly",
        "key_topic": current_message[:30],
        "context_summary": "–¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ",
        "reply_direction": "–ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ",
    }

context_text = "\n".join(
    [
        f"{'–Ø' if msg['is_bot'] else msg['author']}: {msg['content']}"
        for msg in context_messages[-12:]
    ]
)

analysis_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –∏ —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.
–ö–û–ù–¢–ï–ö–°–¢:
{context_text}

–ù–û–í–û–ï –°–û–û–ë–©–ï–ù–ò–ï: {current_message}

–í–µ—Ä–Ω–∏ JSON (–¢–û–õ–¨–ö–û JSON):
{{
"theme": "–æ—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ (2-3 —Å–ª–æ–≤–∞)",
"sentiment": "positive/neutral/negative",
"tone": "friendly/serious/joking/flirty/sarcastic",
"key_topic": "–≥–ª–∞–≤–Ω–∞—è —Ç–µ–º–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞",
"context_summary": "–∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)",
"reply_direction": "–∫–∞–∫ –Ω—É–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å - –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"
}}
"""

text
try:
    response = gemma_model.generate_content(
        analysis_prompt,
        generation_config=genai.types.GenerationConfig(
            temperature=0.3,
            max_output_tokens=300,
        ),
    )

    if not response or not response.text:
        return None

    analysis_text = response.text.strip()

    try:
        analysis = json.loads(analysis_text)
        add_token_usage(gemma_tokens=250)
        return analysis
    except json.JSONDecodeError:
        logger.warning("‚ö†Ô∏è  Gemma –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON")
        return {
            "theme": "–æ–±—â–∏–π —Ä–∞–∑–≥–æ–≤–æ—Ä",
            "sentiment": "neutral",
            "tone": "friendly",
            "key_topic": current_message[:30],
            "context_summary": "–¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ",
            "reply_direction": "–ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ",
        }

except Exception as e:
    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ Gemma: {e}")
    return None
============================================================================
–ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–í–ï–¢–ê (Mistral Large)
============================================================================
async def generate_response(
system_prompt: str,
context_messages: list,
current_message: str,
bot_nick: str,
is_mentioned: bool = False,
chat_phrases: Optional[list] = None,
hot_topics: Optional[list] = None,
user_facts: Optional[list] = None,
mood_state: Optional[str] = None,
energy_level: int = 80,
relationship_level: str = "stranger",
channel_emotes: Optional[list] = None,
) -> Optional[str]:
"""–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."""

text
if not mistral_available or mistral_client is None:
    logger.error("‚ùå Mistral –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–µ –º–æ–≥—É –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç!")
    return None

async with request_lock:
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    logger.info("üß† –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (Google Gemma)...")
    context_analysis = await asyncio.to_thread(
        analyze_context,
        list(context_messages[-config.CONTEXT_MESSAGE_LIMIT :]),
        current_message,
        bot_nick,
    )

    if not context_analysis:
        context_analysis = {
            "theme": "—Ä–∞–∑–≥–æ–≤–æ—Ä",
            "sentiment": "neutral",
            "tone": "friendly",
            "key_topic": current_message[:30],
            "context_summary": "–¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ",
            "reply_direction": "–ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å",
        }

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç–≤–µ—Ç–∞
    if is_mentioned:
        max_length = config.MAX_RESPONSE_LENGTH_MENTIONED
        max_tokens = 500
    else:
        max_length = config.MAX_RESPONSE_LENGTH
        max_tokens = 250

    # –£–ª—É—á—à–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç
    enhanced_prompt = system_prompt
    enhanced_prompt += f"""
[–ê–ù–ê–õ–ò–ó –ö–û–ù–¢–ï–ö–°–¢–ê]:
‚Ä¢ –¢–µ–º–∞: {context_analysis.get('theme', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞')}
‚Ä¢ –¢–æ–Ω: {context_analysis.get('tone', '–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π')}
‚Ä¢ –û —á–µ–º: {context_analysis.get('context_summary', '–Ω–µ—è—Å–Ω–æ')}
"""

text
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    logger.info("‚úçÔ∏è –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç (Mistral Large)...")

    try:
        response = await asyncio.to_thread(
            mistral_client.chat.complete,
            model="mistral-large-latest",
            messages=[
                {"role": "user", "content": enhanced_prompt},
                {"role": "user", "content": current_message},
            ],
            temperature=0.85,
            max_output_tokens=max_tokens,
            top_p=0.9,
        )

        if not response or not response.choices:
            return None

        answer = response.choices.message.content.strip()
        add_token_usage(mistral_tokens=max_tokens)

        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∞–∫—Å –¥–ª–∏–Ω—ã
        if len(answer) > max_length:
            truncated = answer[:max_length]
            last_punct = max(
                truncated.rfind("."),
                truncated.rfind("!"),
                truncated.rfind("?"),
            )
            if last_punct > max_length // 2:
                answer = truncated[: last_punct + 1]
            else:
                answer = truncated.rsplit(" ", 1) + "."

        logger.info("‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
        return answer

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Mistral: {e}")
        return None